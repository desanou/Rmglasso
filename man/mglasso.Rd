% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mglasso.R
\name{mglasso}
\alias{mglasso}
\title{Inference of Multiscale Gaussian Graphical Model.}
\usage{
mglasso(
  x,
  lambda1 = 0,
  fuse_thresh = 0.001,
  maxit = NULL,
  distance = c("euclidean", "relative"),
  lambda2_start = 1e-04,
  lambda2_factor = 1.5,
  precision = 0.01,
  weights_ = NULL,
  type = c("initial"),
  compact = TRUE
)
}
\arguments{
\item{x}{Numeric matrix (\eqn{n \times p}). Multivariate normal sample with
\eqn{n} independent observations.}

\item{lambda1}{Positive numeric scalar. Lasso penalty.}

\item{fuse_thresh}{Positive numeric scalar. Threshold for clusters fusion.}

\item{maxit}{Integer scalar. Maximum number of iterations.}

\item{distance}{Character. Distance between regression vectors with
permutation on symmetric coefficients.}

\item{lambda2_start}{Numeric scalar. Starting value for fused-group Lasso
penalty (clustering penalty).}

\item{lambda2_factor}{Numeric scalar. Step used to update fused-group Lasso
penalty in a multuplicative way..}

\item{precision}{Precision of estimation algorithm.}

\item{weights_}{Matrix of weights.}

\item{type}{If "initial" use classical version of \bold{MGLasso} without
weights.}

\item{compact}{Logical scalar. If TRUE, only save results when previous
clusters are different from current.}
}
\value{
A list. \item{out}{list with matrix of regression vectors and
  clusters for each level. Each element of the list contains two elements:
  \code{beta} and \code{clusters}}. \item{\eqn{\ell_1}}{the sparsity
  penalty}.
}
\description{
Estimates a gaussian graphical model structure while hierarchically grouping
variables.
}
\details{
Estimates a gaussian graphical model structure while hierarchically grouping
variables by optimizing a pseudo-likelihood function combining Lasso and
fuse-group Lasso penalties. The problem is solved via the \emph{COntinuation
with NEsterov smoothing in a Shrinkage-Thresholding Algorithm} (Hadj-selem et
al. 2018). Varying the parameter \eqn{\lambda_2} in a multiplicative fashion
allow to uncover a seemingly hierarchical clustering structure. For
\eqn{\lambda_2 = 0}, the approach is theoretically equivalent to the
Meinshausen-BÃœhlmann (2006) \emph{neighborhood selection} as resuming to the
optimization of \emph{pseudo-likelihood} function with \eqn{\ell_1} penalty
(Rocha et al., 2008). The algorithm stops when all the variables have merged
into one cluster. The criterion used to merge clusters is the
\eqn{\ell_2}-norm distance between regression vectors.
}
\examples{
\dontrun{
n = 50
K = 3
p = 9
rho = 0.85
blocs <- list()
for (j in 1:K) {
  bloc <- matrix(rho, nrow = p/K, ncol = p/K)
  for(i in 1:(p/K)) { bloc[i,i] <- 1 }
  blocs[[j]] <- bloc
}

mat.covariance <- Matrix::bdiag(blocs)
mat.covariance

set.seed(11)
X <- mvtnorm::rmvnorm(n, mean = rep(0,p), sigma = as.matrix(mat.covariance))
X <- scale(X)

res <- mglasso(X, 0.1, lambda2_start = 0.1)
res$out[[1]]$clusters
res$out[[1]]$beta
}
}
\seealso{
\code{\link{conesta}} for the problem solver,
  \code{\link{plot_mglasso}} for plotting the output of \code{mglasso}.
}
